<h3>All About Padding &amp; Pooling in CNN</h3>

<ul>
<li>Deep learning is <em>ALL ABOUT SHRINK DIMS</em></li>
<li>In the shrinking if we foucd on middle but not conner <br />
Things will go bad THAT WE USING PADDING</li>
<li>MaxPooing is proved to functional <em>BUNT DO NOT KNOW WHY</em></li>
</ul>

<h4>Conv2d</h4>

<p><ul>
<li>stride</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>m = nn.Conv2d(16, 33, 3, stride=2) <br />
      m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) <br />
      m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) <br />
      input = torch.randn(20, 16, 50, 100) <br />
      output = m(input)  </li>
      </ul></p>
      
      <h3>Batchnorm  Layernorm</h3>
    </blockquote>
  </blockquote>
</blockquote>

<p><ul>
<li>standard 
<ul>
<li>Batchnorm in CV tasks &amp; Layernorm in NLP tasks.</li>
<li>VGG's normalisation is layer normalisation???</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>How to use batch normalisation &amp; layer normalisation???</li>
      </ul></li>
      </ul></p>
    </blockquote>
  </blockquote>
</blockquote>
