<h3>All About Padding &amp; Pooling in CNN</h3>

<ul>
<li>Deep learning is <em>ALL ABOUT SHRINK DIMS</em></li>
<li>In the shrinking if we foucd on middle but not conner <br />
Things will go bad THAT WE USING PADDING</li>
<li>MaxPooing is proved to functional <em>BUNT DO NOT KNOW WHY</em></li>
</ul>

<h4>Conv2d</h4>

<p><ul>
<li>stride</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>m = nn.Conv2d(16, 33, 3, stride=2) <br />
      m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) <br />
      m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) <br />
      input = torch.randn(20, 16, 50, 100) <br />
      output = m(input)  </li>
      </ul></p>
      
      <h3>Batchnorm  Layernorm</h3>
    </blockquote>
  </blockquote>
</blockquote>

<p><ul>
<li>standard 
<ul>
<li>Batchnorm in CV tasks &amp; Layernorm in NLP tasks.</li>
<li>VGG's normalisation is layer normalisation???</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>How to use <em>batch</em> normalisation &amp; <em>layer</em> normalisation???</li>
      </ul></li>
      </ul></p>
      
      <h3>TextCNN</h3>
    </blockquote>
  </blockquote>
</blockquote>

<h4>Channel_input</h4>

<ul>
<li>输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove）</li>
<li>利用静态词向量和fine-tunning词向量作为不同channel的做法。
<ul>
<li>静态(static)方式：训练过程中不再更新embeddings。实质上属于迁移学习，特别是在目标领域数据量比较小的情况下，采用静态的词向量效果也不错。（通过设置trainable=False）</li>
<li>非静态(non-static)方式：在训练过程中对embeddings进行更新和微调(fine tune)，能加速收敛。（通过设置trainable=True）</li>
</ul></li>
</ul>

<h4>Channel_output</h4>

<ul>
<li>usually 2</li>
</ul>
